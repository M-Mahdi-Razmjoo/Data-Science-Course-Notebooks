{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Mohammad Mahdi Razmjoo - 400101272"
      ],
      "metadata": {
        "id": "UvBlA7TCmi2a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Multilayer Perceptron with Scikit-Learn**"
      ],
      "metadata": {
        "id": "juWr9M9431sD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "binary classification"
      ],
      "metadata": {
        "id": "Iv4Urbik32fg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Binary classification on the Iris dataset\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Load data\n",
        "X, y_multi = load_iris(return_X_y=True)\n",
        "\n",
        "# Convert to binary target: class “setosa” (0) vs. “not-setosa” (1)\n",
        "y = (y_multi != 0).astype(int)\n",
        "\n",
        "# Train / test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test  = scaler.transform(X_test)\n",
        "\n",
        "# Train MLP classifier\n",
        "clf = MLPClassifier(hidden_layer_sizes=(50, 25),\n",
        "                    max_iter=1000,\n",
        "                    early_stopping=True,\n",
        "                    random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate\n",
        "print(\"F1-score:\", f1_score(y_test, clf.predict(X_test)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6z3AUJQt105",
        "outputId": "978f2dbb-783e-4d0d-ec06-dc0ec51d3fde"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1-score: 0.975609756097561\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step-by-Step Explanation & Take-aways\n",
        "\n",
        "| Code section | Why we do it | What we gain / understand |\n",
        "|--------------|--------------|---------------------------|\n",
        "| **Load data**<br>`X, y_multi = load_iris(return_X_y=True)` | Bring the classic Iris measurements (sepal L/W, petal L/W) and their species labels into memory. | We have a small, clean benchmark dataset to test ideas rapidly. |\n",
        "| **Binarize the target**<br>`y = (y_multi != 0).astype(int)` | Collapse the 3-class label to a binary task: *setosa vs. non-setosa*. This simplifies evaluation (single F1-score) and satisfies the assignment’s “binary classification” requirement. | We see how label engineering (re-labelling) can adapt the same data to different problem formulations. |\n",
        "| **Train/test split (80 % / 20 %)**<br>`train_test_split(..., test_size=0.2, stratify=y)` | Reserve 20 % of the samples as an untouched test set. Stratification keeps the class ratio balanced in both splits. | We obtain an unbiased estimate of generalisation performance and obey the rubric’s 20 %-test rule. |\n",
        "| **Feature scaling**<br>`StandardScaler().fit_transform` | The MLP’s gradient descent converges faster and more stably when features are zero-mean and unit-variance. | We experience first-hand how preprocessing affects optimisation and model quality. |\n",
        "| **Define & train the MLP**<br>`MLPClassifier(hidden_layer_sizes=(50,25), ...)` | Build a 4-layer (input → 50 → 25 → 1) feed-forward network, train with early stopping to prevent overfitting. | We practise choosing hidden sizes, epochs and regularisation tricks that matter in real scenarios. |\n",
        "| **Evaluate on the test set**<br>`f1_score(y_test, clf.predict(X_test))` | The F1-score balances precision and recall, making it a robust metric for class-imbalance scenarios. | We verify that the model exceeds the ≥ 0.75 F1 threshold and internalise how metrics inform model acceptance. |\n",
        "\n",
        "### Key understanding\n",
        "* **Data preparation (splits, scaling)** is as critical as the model itself.  \n",
        "* A small network can achieve high performance (≈ 0.95 F1) on a simple, well-separated dataset—showing that model complexity should match data complexity.  \n",
        "* Early stopping turns training into an empirical search for the sweet-spot epoch, illustrating the importance of validation monitoring.\n"
      ],
      "metadata": {
        "id": "bzwYqKmOZ7Mg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "regression"
      ],
      "metadata": {
        "id": "bcg4x0Jb39q9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AWYIbYjPmeeS",
        "outputId": "33aeada1-a118-424a-c257-a93ab681d0f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R²-score: 0.9359089784533952\n"
          ]
        }
      ],
      "source": [
        "# Regression on the same Iris dataset\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Load data (again—but you could reuse X from the first cell)\n",
        "iris   = load_iris()\n",
        "X_full = iris.data\n",
        "# Predict petal length (column index 2) from the other three features\n",
        "y      = X_full[:, 2]\n",
        "X      = X_full[:, [0, 1, 3]]\n",
        "\n",
        "# Train / test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test  = scaler.transform(X_test)\n",
        "\n",
        "# Train MLP regressor\n",
        "regr = MLPRegressor(hidden_layer_sizes=(50, 25),\n",
        "                    max_iter=5000,\n",
        "                    early_stopping=True,\n",
        "                    random_state=42)\n",
        "regr.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate\n",
        "print(\"R²-score:\", r2_score(y_test, regr.predict(X_test)))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step-by-Step Explanation & Take-aways\n",
        "\n",
        "| Code section | Why we do it | What we gain / understand |\n",
        "|--------------|--------------|---------------------------|\n",
        "| **Load data**<br>`iris = load_iris()` | Bring the four numeric Iris measurements into memory. | Same dataset lets us compare classification and regression on identical real data. |\n",
        "| **Select target & predictors**<br>`y = petal length (col 2)`<br>`X = [sepal L, sepal W, petal W]` | Framing a regression task: predict one feature (petal length) from the other three. | Shows how the same table can be sliced into input/target pairs for different problems. |\n",
        "| **Train/test split (80 % / 20 %)**<br>`train_test_split(..., test_size=0.2)` | Hold out 20 % of the samples for unbiased evaluation, matching the rubric. | Reinforces best practice: never judge a model on the data it learned from. |\n",
        "| **Feature scaling**<br>`StandardScaler()` | Centering and scaling accelerates gradient descent and avoids one feature dominating the loss. | Observe the vital role of preprocessing in neural-network optimisation. |\n",
        "| **Define & train the MLP regressor**<br>`MLPRegressor(hidden_layer_sizes=(50,25), …)` | Builds a 4-layer feed-forward network (input → 50 → 25 → 1) and trains with early stopping. | Practice in selecting hidden sizes and regularisation to reach high R² (> 0.8). |\n",
        "| **Evaluate on the test set**<br>`r2_score(...)` | R² measures the proportion of variance explained by the model. | Confirms the network meets the ≥ 0.80 requirement and strengthens intuition for regression metrics. |\n",
        "\n",
        "### Key understanding\n",
        "* **Feature choice shapes difficulty** – predicting petal length from the other three dimensions is easier than from raw species labels; the high R² reveals strong linear/non-linear correlations.  \n",
        "* **Early stopping guards against over-fitting** – training halts when validation loss stops improving, a simple yet powerful regularisation technique.  \n",
        "* **Same architecture, different task** – by swapping loss functions and output activations, a feed-forward network can shift seamlessly between classification and regression."
      ],
      "metadata": {
        "id": "qbLeLH6TaJMV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4-layer feedforward network with Keras**"
      ],
      "metadata": {
        "id": "iTkdkcW23-oX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "binary classification"
      ],
      "metadata": {
        "id": "qVlIGYZf4Blv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4-layer feedforward network – binary classification (setosa vs. non-setosa)\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import f1_score\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# Load data and create binary target\n",
        "X, y_multi = load_iris(return_X_y=True)\n",
        "y = (y_multi != 0).astype(int)\n",
        "\n",
        "# Train / test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test  = scaler.transform(X_test)\n",
        "\n",
        "# Build 4-layer model (3 hidden + 1 output)\n",
        "model = models.Sequential([\n",
        "    layers.Input(shape=(4,)),\n",
        "    layers.Dense(32, activation='relu'),\n",
        "    layers.Dense(16, activation='relu'),\n",
        "    layers.Dense(8,  activation='relu'),\n",
        "    layers.Dense(1,  activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train\n",
        "model.fit(X_train, y_train, epochs=100, batch_size=16, verbose=0)\n",
        "\n",
        "# Evaluate\n",
        "y_pred = (model.predict(X_test) > 0.5).astype(int)\n",
        "print(\"F1-score:\", f1_score(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j0hDV7UAy-0c",
        "outputId": "4252870d-ca98-45f1-c80f-083b121ecfae"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n",
            "F1-score: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step-by-Step Explanation & Take-aways: 4-Layer Keras Feed-Forward Classifier\n",
        "\n",
        "| Code section | Why we do it | What we gain / understand |\n",
        "|--------------|--------------|---------------------------|\n",
        "| **Load data & binarise target**<br>`load_iris()` → `y = (y_multi != 0)` | Convert the 3-class Iris problem into a binary task: *setosa* (0) vs. *non-setosa* (1). | Demonstrates how the same dataset can be reframed for different objectives; simplifies evaluation to a single F1-score. |\n",
        "| **Train/test split (80 % / 20 %)**<br>`train_test_split(..., test_size=0.2, stratify=y)` | Reserve 20 % of samples for an unbiased test set and preserve class ratio with `stratify`. | Ensures the reported F1-score reflects true generalisation, not memorisation. |\n",
        "| **Feature scaling**<br>`StandardScaler()` | Standardise each feature to zero mean and unit variance. Neural nets train faster and more stably when inputs are on comparable scales. | Reinforces the importance of preprocessing for gradient-based optimisation. |\n",
        "| **Define 4-layer model**<br>`Dense(32) → Dense(16) → Dense(8) → Dense(1)` | Create three hidden layers plus one sigmoid output layer, satisfying the “4-layer” criterion. Hidden sizes (32-16-8) strike a balance between capacity and overfitting risk. | Shows the effect of depth and width choices on expressiveness; illustrates that a modest network often suffices for simple tabular data. |\n",
        "| **Compile**<br>`optimizer='adam', loss='binary_crossentropy'` | Adam offers adaptive learning rates; binary cross-entropy matches the Bernoulli target distribution. | Underlines the tight coupling between loss function and task type. |\n",
        "| **Train**<br>`fit(..., epochs=100, batch_size=16)` | Optimise weights for up to 100 epochs with small batches. | Observe how epoch count and batch size influence convergence speed and generalisation. |\n",
        "| **Predict & evaluate**<br>`f1_score(y_test, y_pred)` | Convert probabilities to class labels (`> 0.5`) and compute F1-score, which balances precision and recall. | Confirms the model surpasses the ≥ 0.75 threshold; highlights F1 as a robust metric for potential class imbalance. |\n",
        "\n",
        "### Key understanding\n",
        "* **Architectural simplicity can be sufficient** — a shallow, fully-connected net (> 0.95 F1 in practice) handles the separable Iris data without convolutions or attention.  \n",
        "* **Preprocessing and correct loss choice** are as crucial as the network depth.  \n",
        "* **Hold-out testing** is non-negotiable for honest reporting; the 20 % split enforces this discipline."
      ],
      "metadata": {
        "id": "9W7JaG1KaTBD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "regression"
      ],
      "metadata": {
        "id": "IJWr_Mzy4Jl1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4-layer feedforward network – regression predicting petal length\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import r2_score\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# Load data\n",
        "iris   = load_iris()\n",
        "X_full = iris.data\n",
        "y      = X_full[:, 2]\n",
        "X      = X_full[:, [0, 1, 3]]\n",
        "\n",
        "# Train / test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test  = scaler.transform(X_test)\n",
        "\n",
        "# Build 4-layer model (3 hidden + 1 output)\n",
        "model = models.Sequential([\n",
        "    layers.Input(shape=(3,)),\n",
        "    layers.Dense(32, activation='relu'),\n",
        "    layers.Dense(16, activation='relu'),\n",
        "    layers.Dense(8,  activation='relu'),\n",
        "    layers.Dense(1)\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# Train\n",
        "model.fit(X_train, y_train, epochs=500, batch_size=16, verbose=0)\n",
        "\n",
        "# Evaluate\n",
        "y_pred = model.predict(X_test).flatten()\n",
        "print(\"R²-score:\", r2_score(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k6yJW55zzOLK",
        "outputId": "5bf44efa-5168-456b-9a55-6889a7b3cfb7"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
            "R²-score: 0.9755104234980874\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step-by-Step Explanation & Take-aways: 4-Layer Keras Feed-Forward Regressor\n",
        "\n",
        "| Code section | Why we do it | What we gain / understand |\n",
        "|--------------|--------------|---------------------------|\n",
        "| **Load data**<br>`load_iris()` | Bring the four numeric Iris measurements into memory. | Re-using the same dataset keeps the experimental context consistent across tasks. |\n",
        "| **Define target & predictors**<br>`y = petal length (col 2)`<br>`X = [sepal L, sepal W, petal W]` | Frame a regression problem: predict petal length from the other three dimensions. | Shows how the same table can support a different learning objective by simply re-selecting columns. |\n",
        "| **Train/test split (80 % / 20 %)**<br>`train_test_split(..., test_size=0.2)` | Reserve 20 % of samples for an unbiased evaluation set. | Upholds the rubric’s rule and yields a trustworthy R² estimate. |\n",
        "| **Feature scaling**<br>`StandardScaler()` | Standardise inputs to zero mean and unit variance. Neural nets converge faster and more stably with scaled features. | Reinforces preprocessing as a key ingredient for effective gradient descent. |\n",
        "| **Define 4-layer model**<br>`Dense(32) → 16 → 8 → 1` | Three hidden layers + one linear output layer satisfy the “4-layer” requirement; ReLU activations provide non-linearity. | Highlights how modest depth (and width) can model the non-linear relationships in small tabular data. |\n",
        "| **Compile**<br>`optimizer='adam', loss='mse'` | Adam gives adaptive learning rates; mean-squared error is the standard loss for regression. | Matches the optimisation objective to the task’s statistical assumptions. |\n",
        "| **Train**<br>`fit(..., epochs=500, batch_size=16)` | Allow up to 500 epochs for convergence on the tiny dataset; small batches improve gradient estimates. | Demonstrates that over-training risk is mitigated by early stopping (implicitly via plateauing). |\n",
        "| **Predict & evaluate**<br>`r2_score(y_test, y_pred)` | R² quantifies the proportion of variance explained by the model. | Confirms the net surpasses the ≥ 0.80 hurdle and deepens intuition for regression metrics. |\n",
        "\n",
        "### Key understanding\n",
        "* **Column selection = problem formulation** – by slicing the same matrix differently, we switch seamlessly from classification to regression.  \n",
        "* **Simple feed-forward nets handle small tabular tasks** – high R² (≈ 0.9) shows that even “classic” MLPs remain competitive on low-dimensional data.  \n",
        "* **Preprocessing, loss choice and evaluation protocol** are just as vital as the network architecture for achieving reliable performance."
      ],
      "metadata": {
        "id": "gricrH_Jab0b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4-layer feedforward network with PyTorch**"
      ],
      "metadata": {
        "id": "mt4ACvfk4Lyi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "binary classification"
      ],
      "metadata": {
        "id": "5LqmUcsD4OZL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4-layer feed-forward network – binary classification (setosa vs. non-setosa)\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Reproducibility\n",
        "torch.manual_seed(42);  np.random.seed(42)\n",
        "\n",
        "# ---------- Data ----------\n",
        "X, y_multi = load_iris(return_X_y=True)\n",
        "y = (y_multi != 0).astype(np.float32)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train).astype(np.float32)\n",
        "X_test  = scaler.transform(X_test).astype(np.float32)\n",
        "\n",
        "# Tensors\n",
        "X_train_t = torch.tensor(X_train)\n",
        "y_train_t = torch.tensor(y_train).view(-1, 1)\n",
        "X_test_t  = torch.tensor(X_test)\n",
        "\n",
        "# ---------- Model ----------\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(4, 32),  nn.ReLU(),\n",
        "            nn.Linear(32, 16), nn.ReLU(),\n",
        "            nn.Linear(16,  8), nn.ReLU(),\n",
        "            nn.Linear( 8,  1), nn.Sigmoid()\n",
        "        )\n",
        "    def forward(self, x): return self.net(x)\n",
        "\n",
        "model = Net()\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# ---------- Training ----------\n",
        "for _ in range(300):\n",
        "    optimizer.zero_grad()\n",
        "    loss = criterion(model(X_train_t), y_train_t)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# ---------- Evaluation ----------\n",
        "with torch.no_grad():\n",
        "    preds = (model(X_test_t) > 0.5).cpu().numpy().astype(int)\n",
        "print(\"F1-score:\", f1_score(y_test.astype(int), preds))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CnGQp5q8z1ZK",
        "outputId": "f86da112-9ecc-4602-d12d-100f4814c792"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1-score: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step-by-Step Explanation & Take-aways: 4-Layer PyTorch Feed-Forward Classifier\n",
        "\n",
        "| Code section | Why we do it | What we gain / understand |\n",
        "|--------------|--------------|---------------------------|\n",
        "| **Reproducibility seeds**<br>`torch.manual_seed(42); np.random_seed(42)` | Fix pseudo-random sequences in NumPy and PyTorch. | Ensures results can be replicated and debugging is deterministic. |\n",
        "| **Load & binarise data**<br>`load_iris()` → `y = (y_multi != 0)` | Collapse the 3-class Iris task to *setosa* (0) vs. *non-setosa* (1). | Illustrates label engineering; prepares data for binary cross-entropy loss. |\n",
        "| **Train/test split (80 % / 20 %)**<br>`test_size=0.2, stratify=y` | Hold out 20 % for an unbiased test set and preserve class ratios via stratification. | Satisfies rubric and guarantees fair F1 evaluation. |\n",
        "| **Feature scaling**<br>`StandardScaler()` | Standardise features to zero mean / unit variance to stabilise gradient descent. | Demonstrates the importance of preprocessing for neural-network convergence. |\n",
        "| **Tensor conversion**<br>`torch.tensor(...)` | Move NumPy arrays into PyTorch tensors—PyTorch’s computation primitive. | Enables GPU/CPU tensor arithmetic in the training loop. |\n",
        "| **Define 4-layer network**<br>`Linear 4→32→16→8→1` | Three hidden layers plus a sigmoid output layer = 4 layers total. ReLU brings non-linearity; sigmoid outputs Bernoulli probabilities. | Shows how to express MLP architecture in PyTorch’s `nn.Sequential`. |\n",
        "| **Loss & optimiser**<br>`BCELoss`, `Adam(lr=0.01)` | Binary cross-entropy matches the Bernoulli target; Adam adapts learning rates. | Emphasises pairing the right loss with the task and using modern optimisers to speed convergence. |\n",
        "| **Training loop (300 epochs)** | Manually zero grads, forward pass, backward pass, optimiser step. | Reinforces core mechanics of gradient-based learning and how few epochs suffice for a small dataset. |\n",
        "| **Prediction & evaluation**<br>`(model(X_test_t) > 0.5)` → `f1_score` | Convert probabilities to class labels and compute F1 on the unseen 20 % split. | Validates the model exceeds the ≥ 0.75 threshold (usually ≳ 0.95), illustrating that modest MLPs can perform well on separable tabular data. |\n",
        "\n",
        "### Key understanding\n",
        "* **PyTorch’s explicit training loop** exposes every gradient step, deepening intuition for back-prop mechanics.  \n",
        "* **Preprocessing and architecture** jointly determine performance; a simple 4-layer MLP can excel on linearly-separable data.  \n",
        "* **Reproducibility practices** (random seeds) are crucial when sharing experimental results."
      ],
      "metadata": {
        "id": "qshrVe8QaoEh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "regression"
      ],
      "metadata": {
        "id": "Dmak7FlN4SO7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4-layer feed-forward network – regression predicting petal length\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "torch.manual_seed(42);  np.random.seed(42)\n",
        "\n",
        "# ---------- Data ----------\n",
        "iris   = load_iris()\n",
        "X_full = iris.data.astype(np.float32)\n",
        "y      = X_full[:, 2]\n",
        "X      = X_full[:, [0, 1, 3]]\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train).astype(np.float32)\n",
        "X_test  = scaler.transform(X_test).astype(np.float32)\n",
        "\n",
        "X_train_t = torch.tensor(X_train)\n",
        "y_train_t = torch.tensor(y_train).view(-1, 1)\n",
        "X_test_t  = torch.tensor(X_test)\n",
        "\n",
        "# ---------- Model ----------\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(3, 32),  nn.ReLU(),\n",
        "            nn.Linear(32, 16), nn.ReLU(),\n",
        "            nn.Linear(16,  8), nn.ReLU(),\n",
        "            nn.Linear( 8,  1)\n",
        "        )\n",
        "    def forward(self, x): return self.net(x)\n",
        "\n",
        "model = Net()\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# ---------- Training ----------\n",
        "for _ in range(1000):\n",
        "    optimizer.zero_grad()\n",
        "    loss = criterion(model(X_train_t), y_train_t)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# ---------- Evaluation ----------\n",
        "with torch.no_grad():\n",
        "    preds = model(X_test_t).cpu().numpy().flatten()\n",
        "print(\"R²-score:\", r2_score(y_test, preds))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3QSDg1U0z5Sa",
        "outputId": "ba00b577-11ed-467d-a89c-a9d099dd2dd2"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R²-score: 0.9644560217857361\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step-by-Step Explanation & Take-aways: 4-Layer PyTorch Feed-Forward Regressor\n",
        "\n",
        "| Code section | Why we do it | What we gain / understand |\n",
        "|--------------|--------------|---------------------------|\n",
        "| **Seeds for reproducibility**<br>`torch.manual_seed(42); np.random.seed(42)` | Fix random number generators in NumPy & PyTorch. | Guarantees results can be replicated, debugging is deterministic. |\n",
        "| **Load & slice data**<br>`load_iris()` → `y = petal length`, `X = [sepal L, sepal W, petal W]` | Formulate a regression problem: predict petal length from the other three numeric variables. | Shows that simple column selection can turn the same dataset into a new task. |\n",
        "| **Train/test split (80 % / 20 %)**<br>`train_test_split(..., test_size=0.2)` | Reserve 20 % of samples for a held-out evaluation set, matching rubric. | Provides an unbiased R² estimate of model generalisation. |\n",
        "| **Feature scaling**<br>`StandardScaler()` | Standardise inputs to zero mean and unit variance to stabilise gradient descent. | Highlights how preprocessing accelerates learning and prevents feature-scale dominance. |\n",
        "| **Tensor conversion**<br>`torch.tensor(...)` | Move NumPy arrays into PyTorch tensors, the library’s computation objects. | Enables GPU/CPU tensor math in the training loop. |\n",
        "| **Define 4-layer network**<br>`Linear 3→32→16→8→1` with ReLU in hidden layers | Three hidden layers plus one linear output layer (4 layers total). ReLU provides non-linearity. | Demonstrates PyTorch’s modular `nn.Sequential` for MLP architecture. |\n",
        "| **Loss & optimiser**<br>`MSELoss`, `Adam(lr=0.01)` | Mean-squared error is standard for regression; Adam adapts learning rates for faster convergence. | Emphasises pairing appropriate loss functions and optimisers with the task. |\n",
        "| **Training loop (1 000 epochs)** | Explicitly zero gradients, forward pass, back-propagate, and update weights. | Builds intuition for every step of gradient descent; shows how many passes a tiny dataset may need. |\n",
        "| **Prediction & evaluation**<br>`r2_score(y_test, preds)` | Compute R² on the unseen 20 % split to measure variance explained. | Verifies the model surpasses the ≥ 0.80 requirement (typically ≳ 0.90) and cements understanding of regression metrics. |\n",
        "\n",
        "### Key understanding\n",
        "* **Column selection drives task definition** – with only slicing, we jump from classification to regression.  \n",
        "* **Explicit training loops in PyTorch** reveal the mechanics of back-prop and how learning rates and epochs affect convergence.  \n",
        "* **Preprocessing remains crucial** even for small networks; proper scaling can be the difference between divergence and a high R².  \n",
        "* **Modest MLPs are often enough** – a 4-layer network easily captures the non-linear correlations within low-dimensional tabular data."
      ],
      "metadata": {
        "id": "VbjI5_B0a5jx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4-layer non-sequential feedforward network with Keras**"
      ],
      "metadata": {
        "id": "VCuzw2l34Xiq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "binary classification"
      ],
      "metadata": {
        "id": "CUp60_cL4aCU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4-layer non-sequential network – binary classification (setosa vs. non-setosa)\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import f1_score\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Model, Input\n",
        "\n",
        "# ---------- Data ----------\n",
        "X, y_multi = load_iris(return_X_y=True)\n",
        "y = (y_multi != 0).astype(int)\n",
        "X_tr, X_te, y_tr, y_te = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "scaler = StandardScaler()\n",
        "X_tr = scaler.fit_transform(X_tr);  X_te = scaler.transform(X_te)\n",
        "\n",
        "# ---------- Model (Functional API, 4 layers total) ----------\n",
        "inp = Input(shape=(4,))\n",
        "x   = layers.Dense(32, activation=\"relu\")(inp)\n",
        "x   = layers.Dense(16, activation=\"relu\")(x)\n",
        "x   = layers.Dense(8,  activation=\"relu\")(x)\n",
        "out = layers.Dense(1,  activation=\"sigmoid\")(x)\n",
        "model = Model(inputs=inp, outputs=out)\n",
        "\n",
        "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "model.fit(X_tr, y_tr, epochs=100, batch_size=16, verbose=0)\n",
        "\n",
        "# ---------- Evaluation ----------\n",
        "y_pred = (model.predict(X_te) > 0.5).astype(int)\n",
        "print(\"F1-score:\", f1_score(y_te, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fMK3AHlG0JLJ",
        "outputId": "016896bc-ca32-4644-dd6c-0824a84cda66"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step\n",
            "F1-score: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step-by-Step Explanation & Take-aways: 4-Layer Keras Functional-API Classifier\n",
        "\n",
        "| Code section | Why we do it | What we gain / understand |\n",
        "|--------------|--------------|---------------------------|\n",
        "| **Load & binarise data**<br>`load_iris()` → `y = (y_multi != 0)` | Turn the 3-class Iris task into a binary one: *setosa* vs. *non-setosa*. | Demonstrates label engineering; simplifies evaluation to a single F1-score. |\n",
        "| **Train/test split (80 % / 20 %)**<br>`train_test_split(..., stratify=y)` | Keep 20 % as an untouched test set and preserve the class ratio by stratifying. | Provides an unbiased generalisation metric and satisfies the rubric’s 20 % rule. |\n",
        "| **Feature scaling**<br>`StandardScaler()` | Standardise features to zero mean and unit variance for faster, stabler gradient descent. | Reinforces the importance of preprocessing in neural-network optimisation. |\n",
        "| **Define 4-layer model with Functional API**<br>`Input → Dense32 → Dense16 → Dense8 → Dense1(sigmoid)` | Three hidden layers plus one sigmoid output layer (4 layers total). Using Functional API (non-sequential) illustrates flexible graph construction—inputs/outputs are explicitly wired. | Shows how to create DAG-style models (skip connections, multi-input, etc.) beyond what `Sequential` allows. |\n",
        "| **Compile**<br>`optimizer=\"adam\", loss=\"binary_crossentropy\"` | Adam provides adaptive learning rates; binary cross-entropy matches the Bernoulli target distribution. | Underlines the direct link between task type and loss function. |\n",
        "| **Train**<br>`fit(X_tr, y_tr, epochs=100, batch_size=16)` | Optimise the network weights over 100 epochs with mini-batches of 16. | Observes that modest training budgets suffice for small, separable tabular data. |\n",
        "| **Predict & evaluate**<br>`f1_score(y_te, y_pred)` | Convert probabilities to class labels (`>0.5`) and compute F1 on the 20 % hold-out. | Confirms the model surpasses the ≥ 0.75 requirement (typically ≳ 0.95), highlighting F1 as a balanced metric when class distribution might be skewed. |\n",
        "\n",
        "### Key understanding\n",
        "* **Functional API unlocks architecture flexibility**—while this example is a straight feed-forward graph, the same pattern extends to multi-branch or residual designs.  \n",
        "* **Model depth vs. dataset complexity**—a shallow 4-layer net is enough for the well-separated Iris features; deeper or wider nets would likely overfit.  \n",
        "* **Data handling, loss selection and evaluation protocol** are as critical as the network definition in achieving trustworthy performance."
      ],
      "metadata": {
        "id": "7a0URycgbB4b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "regression"
      ],
      "metadata": {
        "id": "ajux3Wx64cIT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4-layer non-sequential network – regression predicting petal length\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import r2_score\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Model, Input\n",
        "\n",
        "# ---------- Data ----------\n",
        "iris = load_iris()\n",
        "X    = iris.data.astype(float)[:, [0, 1, 3]]\n",
        "y    = iris.data.astype(float)[:, 2]\n",
        "X_tr, X_te, y_tr, y_te = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "scaler = StandardScaler()\n",
        "X_tr = scaler.fit_transform(X_tr);  X_te = scaler.transform(X_te)\n",
        "\n",
        "# ---------- Model (Functional API, 4 layers total) ----------\n",
        "inp = Input(shape=(3,))\n",
        "x   = layers.Dense(32, activation=\"relu\")(inp)\n",
        "x   = layers.Dense(16, activation=\"relu\")(x)\n",
        "x   = layers.Dense(8,  activation=\"relu\")(x)\n",
        "out = layers.Dense(1)(x)\n",
        "model = Model(inputs=inp, outputs=out)\n",
        "\n",
        "model.compile(optimizer=\"adam\", loss=\"mse\")\n",
        "model.fit(X_tr, y_tr, epochs=500, batch_size=16, verbose=0)\n",
        "\n",
        "# ---------- Evaluation ----------\n",
        "y_pred = model.predict(X_te).flatten()\n",
        "print(\"R²-score:\", r2_score(y_te, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3OtIfzYx0Ny4",
        "outputId": "72d74930-3879-421b-e770-6f323c657e28"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 133 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7fb25ba85da0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step\n",
            "R²-score: 0.973116774108986\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step-by-Step Explanation & Take-aways: 4-Layer Keras Functional-API Regressor\n",
        "\n",
        "| Code section | Why we do it | What we gain / understand |\n",
        "|--------------|--------------|---------------------------|\n",
        "| **Load & slice data**<br>`load_iris()` → `X = [sepal L, sepal W, petal W]`, `y = petal length` | Frame a regression task: predict petal length (target) from the other three numeric measurements (features). | Demonstrates how simple column selection turns the same dataset into a new supervised-learning problem. |\n",
        "| **Train/test split (80 % / 20 %)**<br>`train_test_split(..., test_size=0.2)` | Reserve 20 % of the samples for unbiased evaluation. | Satisfies the rubric’s rule and yields a trustworthy R² estimate of generalisation. |\n",
        "| **Feature scaling**<br>`StandardScaler()` | Standardise inputs to zero mean and unit variance to stabilise gradient descent. | Reinforces that preprocessing is vital for neural-network convergence, especially on small tabular data. |\n",
        "| **Define 4-layer model (Functional API)**<br>`Input → Dense32 → Dense16 → Dense8 → Dense1` | Three hidden ReLU layers plus one linear output layer (4 layers total). Functional API allows flexible, non-sequential graph construction. | Shows how to build feed-forward nets that could easily be extended with skip or multi-branch connections. |\n",
        "| **Compile**<br>`optimizer=\"adam\", loss=\"mse\"` | Adam supplies adaptive learning rates; mean-squared error is the canonical loss for regression. | Emphasises pairing the correct loss function with the task type. |\n",
        "| **Train**<br>`fit(..., epochs=500, batch_size=16)` | Optimise network weights for up to 500 epochs with mini-batches of 16. | Illustrates that small datasets often converge quickly; extra epochs do little harm thanks to early plateauing. |\n",
        "| **Predict & evaluate**<br>`r2_score(y_te, y_pred)` | Compute R² on the 20 % hold-out to measure variance explained by the model. | Confirms the network exceeds the ≥ 0.80 requirement (typically ≳ 0.90) and builds intuition for regression metrics. |\n",
        "\n",
        "### Key understanding\n",
        "* **Functional API > Sequential for flexibility** – even though the graph is linear here, the same pattern enables complex architectures (multi-input, residual, etc.).  \n",
        "* **Small MLPs can model modest nonlinearities** – a 4-layer net captures most variance in low-dimensional tabular data without risk of severe overfitting.  \n",
        "* **Proper preprocessing and evaluation protocol** are just as critical as architecture: scaling the features and holding out a test split underpin the high R² score."
      ],
      "metadata": {
        "id": "R09UKVXwbJIK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Neural networks are powerful because they can approximate almost any function, build hierarchical feature representations directly from raw data, and scale effectively with modern hardware—but designing them is hard because the vast space of architectures and training settings offers few guarantees and many pitfalls.**\n",
        "\n",
        "## Why neural networks are so powerful\n",
        "1. **Universal approximation** – A feed-forward network with even one hidden layer can approximate any continuous function on a compact domain (the universal-approximation theorem).  \n",
        "2. **Depth builds hierarchies** – Stacking layers lets a model reuse simple patterns to construct higher-level features, which gives deep nets far greater expressive power per parameter than shallow ones.  \n",
        "3. **End-to-end feature learning** – Back-propagation trains all layers jointly, so the network discovers task-specific features without manual engineering.  \n",
        "4. **Scalability with compute** – GPUs/TPUs allow massive parallelism, so networks grow to billions of parameters while training times stay practical.  \n",
        "5. **Cross-domain versatility** – The same core idea (with tweaks like convolutions or attention) solves vision, language, audio, and time-series tasks.\n",
        "\n",
        "## What’s difficult about designing neural networks\n",
        "1. **Huge design space** – Choosing depth, width, activations, optimizers, learning rates, regularizers, etc. is largely empirical; exhaustive search is expensive.  \n",
        "2. **Training instabilities** – Deep nets can suffer vanishing/exploding gradients; careful initialization, normalization, and residual connections only partially tame this.  \n",
        "3. **Overfitting vs. generalization** – Powerful models memorize easily; avoiding this demands big data, augmentation, and regularization strategies.  \n",
        "4. **Compute and energy cost** – Cutting-edge models require substantial hardware, time, and power, limiting accessibility and raising environmental concerns.  \n",
        "5. **Interpretability & safety** – Networks behave like black boxes; it remains hard to explain or verify their decisions, complicating debugging and trust."
      ],
      "metadata": {
        "id": "Z00YAWeK3PNe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3-layer Recurrent Neural Network with Keras**"
      ],
      "metadata": {
        "id": "xZgwsEX-4hX7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bonus**\n",
        "\n",
        "Although the Iris dataset is not a true time-series, we treat each feature as a “time-step” so the data can flow through an RNN while still meeting the “same dataset for all parts” requirement."
      ],
      "metadata": {
        "id": "WXW1y_4o1jbu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "binary classification"
      ],
      "metadata": {
        "id": "g3UXFQhY4pjV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3-layer LSTM network – binary classification (setosa vs. non-setosa)\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import f1_score\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# ---------- Data ----------\n",
        "X, y_multi = load_iris(return_X_y=True)\n",
        "y = (y_multi != 0).astype(int)\n",
        "\n",
        "# Scale each of the four “time-steps”\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Reshape so each feature becomes a time-step: (samples, timesteps, features)\n",
        "X_seq = X_scaled.reshape(X_scaled.shape[0], 4, 1)\n",
        "\n",
        "X_tr, X_te, y_tr, y_te = train_test_split(\n",
        "    X_seq, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# ---------- Model (3 recurrent layers + 1 dense) ----------\n",
        "inp = Input(shape=(4, 1))\n",
        "x   = LSTM(32, return_sequences=True)(inp)\n",
        "x   = LSTM(16, return_sequences=True)(x)\n",
        "x   = LSTM(8)(x)\n",
        "out = Dense(1, activation=\"sigmoid\")(x)\n",
        "model = Model(inp, out)\n",
        "\n",
        "model.compile(optimizer=\"adam\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.fit(X_tr, y_tr, epochs=100, batch_size=16, verbose=0)\n",
        "\n",
        "# ---------- Evaluation ----------\n",
        "y_pred = (model.predict(X_te) > 0.5).astype(int)\n",
        "print(\"F1-score:\", f1_score(y_te, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQBmpyUl03fT",
        "outputId": "d6609eea-009c-4572-8b1a-84aed4aeae5d"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:6 out of the last 134 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7fb258ec4c20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 531ms/step\n",
            "F1-score: 0.9523809523809523\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step-by-Step Explanation & Take-aways: 3-Layer LSTM Classifier (Iris, Binary)\n",
        "\n",
        "| Code section | Why we do it | What we gain / understand |\n",
        "|--------------|--------------|---------------------------|\n",
        "| **Load & binarise data**<br>`load_iris()` → `y = (y_multi != 0)` | Collapse the 3-class Iris labels to *setosa* (0) vs. *non-setosa* (1). | Illustrates how label engineering adapts a dataset to a binary problem. |\n",
        "| **Feature scaling**<br>`StandardScaler()` | Standardise each feature before feeding it to the network. | Prevents one time-step (feature) from dominating gradients and speeds convergence. |\n",
        "| **Reshape to sequence**<br>`X_seq.reshape(n_samples, 4, 1)` | Treat the **4 features as 4 time-steps** in a univariate sequence so an LSTM can process them. | Shows a creative way to shoe-horn tabular data into an RNN when no genuine temporal dimension exists, satisfying the “time-series-like” requirement. |\n",
        "| **Train/test split (80 % / 20 %)**<br>`train_test_split(..., stratify=y)` | Keep 20 % of samples for an unbiased test set; stratification preserves class balance. | Ensures F1 is measured on unseen data per rubric. |\n",
        "| **Define 3-layer LSTM + dense output**<br>`LSTM32 → LSTM16 → LSTM8 → Dense1(sigmoid)` | Three recurrent layers model sequential dependencies; final sigmoid outputs a probability. | Demonstrates stacking LSTMs (deep RNN) and finishing with a dense layer for binary classification. |\n",
        "| **Compile**<br>`optimizer=\"adam\", loss=\"binary_crossentropy\"` | Adam adapts learning rates; binary cross-entropy matches Bernoulli targets. | Reinforces correct loss choice for classification probabilities. |\n",
        "| **Train**<br>`fit(..., epochs=100, batch_size=16)` | Optimise weights on 80 % training data for 100 epochs. | Illustrates that even small RNNs can converge quickly on miniature datasets. |\n",
        "| **Predict & evaluate**<br>`f1_score(y_te, y_pred)` | Threshold probabilities at 0.5, compute F1 on the 20 % hold-out. | Confirms the network exceeds the ≥ 0.75 F1 requirement (typically ≳ 0.95) and highlights F1’s robustness to potential class imbalance. |\n",
        "\n",
        "### Key understanding\n",
        "* **Sequentialising tabular data**—re-interpreting features as time-steps—lets us satisfy RNN requirements when no real chronology exists.  \n",
        "* **Layer depth in RNNs** can boost representational capacity, yet small LSTMs already achieve high F1 on well-separated data.  \n",
        "* **Evaluation discipline** (20 % test split) remains paramount: without it, high train metrics could mask poor generalisation."
      ],
      "metadata": {
        "id": "HFxQMasxbRoS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "regression"
      ],
      "metadata": {
        "id": "vvDrGnuH4qoo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3-layer LSTM network – regression predicting petal length\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import r2_score\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# ---------- Data ----------\n",
        "iris = load_iris()\n",
        "# Predict petal length (feature 2) from the other three features (0,1,3)\n",
        "y = iris.data[:, 2]\n",
        "X = iris.data[:, [0, 1, 3]]\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Reshape to (samples, timesteps=3, features=1)\n",
        "X_seq = X_scaled.reshape(X_scaled.shape[0], 3, 1)\n",
        "\n",
        "X_tr, X_te, y_tr, y_te = train_test_split(\n",
        "    X_seq, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# ---------- Model (3 recurrent layers + 1 dense) ----------\n",
        "inp = Input(shape=(3, 1))\n",
        "x   = LSTM(32, return_sequences=True)(inp)\n",
        "x   = LSTM(16, return_sequences=True)(x)\n",
        "x   = LSTM(8)(x)\n",
        "out = Dense(1)(x)\n",
        "model = Model(inp, out)\n",
        "\n",
        "model.compile(optimizer=\"adam\", loss=\"mse\")\n",
        "model.fit(X_tr, y_tr, epochs=500, batch_size=16, verbose=0)\n",
        "\n",
        "# ---------- Evaluation ----------\n",
        "y_pred = model.predict(X_te).flatten()\n",
        "print(\"R²-score:\", r2_score(y_te, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQJSa3LE07cU",
        "outputId": "021ab384-73ac-46eb-c074-97091c7460e9"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 445ms/step\n",
            "R²-score: 0.974721401952081\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step-by-Step Explanation & Take-aways: 3-Layer LSTM Regressor (Iris, Petal-Length)\n",
        "\n",
        "| Code section | Why we do it | What we gain / understand |\n",
        "|--------------|--------------|---------------------------|\n",
        "| **Load & slice data**<br>`load_iris()` → `y = petal length`, `X = [sepal L, sepal W, petal W]` | Frame a regression task: predict petal length from the other three numeric measurements. | Demonstrates how the same matrix can be partitioned to define a new supervised problem. |\n",
        "| **Feature scaling**<br>`StandardScaler()` | Standardise each predictor to zero mean / unit variance. | Prevents scale disparities from skewing gradient magnitudes and accelerates convergence. |\n",
        "| **Reshape to sequences**<br>`X_scaled.reshape(n_samples, 3, 1)` | Treat the **3 predictors as 3 time-steps** in a single-feature sequence (shape = *timesteps*, *features*). | Satisfies the “time-series-like” requirement even though the data are tabular; lets an RNN process it. |\n",
        "| **Train/test split (80 % / 20 %)**<br>`train_test_split(..., test_size=0.2)` | Hold out 20 % of samples for an unbiased R² evaluation. | Meets rubric and ensures we assess true generalisation, not memorisation. |\n",
        "| **Define 3-layer LSTM + linear head**<br>`LSTM32 → LSTM16 → LSTM8 → Dense1` | Stack three recurrent layers (32, 16, 8 units) followed by a dense layer producing a scalar. | Shows how deep RNNs can model sequential dependencies (here, inter-feature relations). |\n",
        "| **Compile**<br>`optimizer=\"adam\", loss=\"mse\"` | Adam adapts learning rates; mean-squared error is standard for regression. | Reinforces alignment of loss with task type. |\n",
        "| **Train**<br>`fit(..., epochs=500, batch_size=16)` | Optimise weights for up to 500 epochs on the 80 % training portion. | Illustrates that small RNNs converge quickly on low-dimensional data; lengthy epochs stabilize learning. |\n",
        "| **Predict & evaluate**<br>`r2_score(y_te, y_pred)` | Compute R² on the unseen 20 % subset: proportion of variance explained. | Confirms the network exceeds the ≥ 0.80 requirement (commonly ≳ 0.90) and deepens intuition for regression metrics. |\n",
        "\n",
        "### Key understanding\n",
        "* **Sequentialising features** lets us apply RNNs to otherwise static tabular data, offering a vantage point on cross-feature dependencies.  \n",
        "* **Layer depth in LSTMs** improves representational capacity; even so, modest widths suffice for this simple dataset.  \n",
        "* **Preprocessing & evaluation discipline** remain vital: scaling and a strict 20 % hold-out underpin the high R² score and prevent misleading results."
      ],
      "metadata": {
        "id": "dAaWlM9-bZwJ"
      }
    }
  ]
}