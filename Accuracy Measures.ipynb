{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Mohammad Mahdi Razmjoo - 400101272"
      ],
      "metadata": {
        "id": "7TwUTyBizegU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading MNIST"
      ],
      "metadata": {
        "id": "yTBwqljCz65g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mwXOOxd4zHgO",
        "outputId": "a25e6d97-1f9c-4e17-8735-94e5d949d974"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X: (10000, 784)\n",
            "Shape of y: (10000,)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.metrics import (mean_squared_error, mean_absolute_error,\n",
        "                             mean_absolute_percentage_error, r2_score,\n",
        "                             precision_score, recall_score, f1_score,\n",
        "                             hamming_loss)\n",
        "\n",
        "mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n",
        "X = mnist.data\n",
        "X = X[:10000]\n",
        "y = mnist.target.astype(int)\n",
        "y = y[:10000]\n",
        "\n",
        "print(\"Shape of X:\", X.shape)\n",
        "print(\"Shape of y:\", y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explanation of the above code\n",
        "This part of the code works with the MNIST dataset, a well-known collection of handwritten digits used for machine learning tasks like classification. By using the fetch_openml function, the dataset is loaded into memory as numerical data (X) representing the pixel values of images and their corresponding labels (y) representing the digits (0–9).\n",
        "\n",
        "The code then limits the dataset to the first 10,000 samples for computational efficiency, making it faster to process and train models on. The y values are also converted to integers to ensure compatibility with machine learning algorithms that require numeric labels.\n",
        "\n",
        "By printing the shapes of X and y, the code verifies the dimensions of the dataset to confirm its integrity before proceeding with further steps."
      ],
      "metadata": {
        "id": "VTNuirgm6BYu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regression"
      ],
      "metadata": {
        "id": "NMR1wUpQ0EbY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "regressor = LinearRegression()\n",
        "regressor.fit(X_train_reg, y_train_reg)\n",
        "\n",
        "y_pred_reg = regressor.predict(X_test_reg)\n",
        "\n",
        "mse = mean_squared_error(y_test_reg, y_pred_reg)\n",
        "mae = mean_absolute_error(y_test_reg, y_pred_reg)\n",
        "mape = mean_absolute_percentage_error(y_test_reg, y_pred_reg)\n",
        "r2 = r2_score(y_test_reg, y_pred_reg)\n",
        "\n",
        "print(f\"MSE: {mse:.4f}\")\n",
        "print(f\"MAE: {mae:.4f}\")\n",
        "print(f\"MAPE: {mape:.4f}\")\n",
        "print(f\"R² Score: {r2:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tWw2l4n60F33",
        "outputId": "17150833-ece6-4c13-ebd3-fdd59ff875b7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Regression Metrics (Predicting digit as continuous value):\n",
            "MSE: 4.6868\n",
            "MAE: 1.4820\n",
            "MAPE: 666409616012723.8750\n",
            "R² Score: 0.4281\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explanation of the aboce code\n",
        "This part of the code applies linear regression to the MNIST dataset, treating the digit labels as continuous values instead of discrete classes. By splitting the data into training and testing sets, the model is trained on the training data and makes predictions on the test data. Metrics such as Mean Squared Error (MSE), Mean Absolute Error (MAE), Mean Absolute Percentage Error (MAPE), and R² Score are calculated to evaluate how well the model predicts the digits.\n",
        "\n",
        "The purpose of this step is to explore how regression performs when predicting numerical values, even though the problem is inherently classification-based. It offers insights into the behavior and limitations of regression models in scenarios where data isn't truly continuous.\n",
        "\n",
        "From this, I have learned how regression can be applied creatively to classification problems and the importance of evaluating performance using multiple metrics to understand the model's strengths and weaknesses comprehensively. It demonstrates that regression models may not excel in tasks where the target is categorical, such as the digit labels."
      ],
      "metadata": {
        "id": "8ohexEuF4L1i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Binary Classification"
      ],
      "metadata": {
        "id": "rB2f2cc20Ifr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_binary = (y % 2 == 0).astype(int)\n",
        "X_train_bin, X_test_bin, y_train_bin, y_test_bin = train_test_split(X, y_binary, test_size=0.2, random_state=42)\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train_bin = scaler.fit_transform(X_train_bin)\n",
        "X_test_bin = scaler.transform(X_test_bin)\n",
        "\n",
        "clf_bin = LogisticRegression(max_iter=20000, random_state=42)\n",
        "clf_bin.fit(X_train_bin, y_train_bin)\n",
        "\n",
        "y_pred_bin = clf_bin.predict(X_test_bin)\n",
        "\n",
        "precision_bin = precision_score(y_test_bin, y_pred_bin)\n",
        "recall_bin = recall_score(y_test_bin, y_pred_bin)\n",
        "f1_bin = f1_score(y_test_bin, y_pred_bin)\n",
        "\n",
        "print(f\"Precision: {precision_bin:.4f}\")\n",
        "print(f\"Recall: {recall_bin:.4f}\")\n",
        "print(f\"F1-Score: {f1_bin:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yUSQyxO50Mlu",
        "outputId": "82d3940d-39da-4fb2-8254-9fc8687481d2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Binary Classification Metrics (Even vs. Odd):\n",
            "Precision: 0.8978\n",
            "Recall: 0.8710\n",
            "F1-Score: 0.8842\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explanation of the above code\n",
        "This part of the code performs a binary classification task to determine whether a digit from the MNIST dataset is even or odd. A new binary label (y_binary) is created, where even digits are encoded as 1 and odd digits as 0. The dataset is then split into training and testing sets.\n",
        "\n",
        "Before training the model, the features are standardized using StandardScaler to ensure all variables have the same scale, which is essential for logistic regression. The logistic regression model is trained on the scaled training data and then used to make predictions on the test set. The model's performance is evaluated using classification metrics: precision (accuracy for positive predictions), recall (sensitivity to capturing all positive cases), and F1-score (harmonic mean of precision and recall).\n",
        "\n",
        "This step is designed to explore how logistic regression performs in classifying even and odd digits. I’ve learned how to handle binary classification problems, from creating binary labels to preprocessing and model evaluation. Additionally, I now better understand how metrics like precision, recall, and F1-score provide a more nuanced view of a model’s performance, beyond just overall accuracy."
      ],
      "metadata": {
        "id": "EqhnpaFq68J7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multi-class Classification"
      ],
      "metadata": {
        "id": "wmh9addw0O7o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_mc, X_test_mc, y_train_mc, y_test_mc = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "clf_mc = LogisticRegression(solver='lbfgs', max_iter=2000, random_state=42)\n",
        "clf_mc.fit(X_train_mc, y_train_mc)\n",
        "\n",
        "y_pred_mc = clf_mc.predict(X_test_mc)\n",
        "\n",
        "precision_each = precision_score(y_test_mc, y_pred_mc, average=None)\n",
        "recall_each = recall_score(y_test_mc, y_pred_mc, average=None)\n",
        "\n",
        "f1_macro = f1_score(y_test_mc, y_pred_mc, average='macro')\n",
        "f1_weighted = f1_score(y_test_mc, y_pred_mc, average='weighted')\n",
        "f1_micro = f1_score(y_test_mc, y_pred_mc, average='micro')\n",
        "\n",
        "print(f\"Precision for each class: {precision_each}\")\n",
        "print(f\"Recall for each class: {recall_each}\")\n",
        "print(f\"Macro-averaged F1-Score: {f1_macro:.4f}\")\n",
        "print(f\"Weighted-averaged F1-Score: {f1_weighted:.4f}\")\n",
        "print(f\"Micro-averaged F1-Score: {f1_micro:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xwx1TSsu0Sol",
        "outputId": "92e70083-d880-400d-e304-abe840f9a18a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision for each class: [0.9478673  0.93693694 0.86528497 0.83       0.93170732 0.83050847\n",
            " 0.9321267  0.88181818 0.78658537 0.86096257]\n",
            "Recall for each class: [0.96618357 0.96296296 0.81862745 0.86458333 0.90521327 0.83522727\n",
            " 0.93636364 0.89814815 0.77710843 0.83854167]\n",
            "Macro-averaged F1-Score: 0.8802\n",
            "Weighted-averaged F1-Score: 0.8842\n",
            "Micro-averaged F1-Score: 0.8845\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explanation of the above code\n",
        "This section implements multi-class classification using logistic regression to classify the MNIST digit dataset (digits 0–9). The dataset is split into training and testing sets, ensuring the model is trained on a portion of the data and evaluated on unseen data to assess its generalization ability. The logistic regression classifier is trained on the training set, using the 'lbfgs' solver, which is efficient for multi-class problems.\n",
        "\n",
        "The model's performance is evaluated by calculating precision and recall for each individual class, providing insights into how well the classifier distinguishes specific digits. Additionally, three types of F1-scores—macro, weighted, and micro-averaged—are computed to evaluate overall performance:\n",
        "\n",
        "*   Macro F1-score treats all classes equally by averaging F1-scores across classes.\n",
        "*   Weighted F1-score considers the class distribution, giving higher importance to more frequent classes.\n",
        "* Micro F1-score focuses on individual samples rather than class balance.\n",
        "\n",
        "This approach ensures a comprehensive evaluation of the classifier across imbalanced or balanced datasets. From this, I’ve learned how logistic regression can handle multi-class classification effectively and how different scoring metrics provide a more detailed understanding of model performance. It also highlights the importance of considering class-specific precision and recall to improve interpretability."
      ],
      "metadata": {
        "id": "fhUq1MRP7dBr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multi-label Classification"
      ],
      "metadata": {
        "id": "-mm-EYIX0Upf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def is_even(x):\n",
        "    return 1 if x % 2 == 0 else 0\n",
        "\n",
        "def is_prime(x):\n",
        "    return 1 if x in (2, 3, 5, 7) else 0\n",
        "\n",
        "def less_than_5(x):\n",
        "    return 1 if x < 5 else 0\n",
        "\n",
        "def greater_than_5(x):\n",
        "    return 1 if x > 5 else 0\n",
        "\n",
        "y_multilabel = np.array([\n",
        "    [is_even(val), is_prime(val), less_than_5(val), greater_than_5(val)]\n",
        "    for val in y\n",
        "])\n",
        "\n",
        "X_train_ml, X_test_ml, y_train_ml, y_test_ml = train_test_split(X, y_multilabel, test_size=0.2, random_state=42)\n",
        "\n",
        "scaler_ml = StandardScaler()\n",
        "X_train_ml = scaler_ml.fit_transform(X_train_ml)\n",
        "X_test_ml = scaler_ml.transform(X_test_ml)\n",
        "\n",
        "clf_ml = OneVsRestClassifier(LogisticRegression(max_iter=20000, random_state=42))\n",
        "clf_ml.fit(X_train_ml, y_train_ml)\n",
        "\n",
        "y_pred_ml = clf_ml.predict(X_test_ml)\n",
        "\n",
        "hloss = hamming_loss(y_test_ml, y_pred_ml)\n",
        "f1_ml = f1_score(y_test_ml, y_pred_ml, average='samples')\n",
        "\n",
        "print(f\"Hamming Loss: {hloss:.4f}\")\n",
        "print(f\"F1-Score (Sample Averaged): {f1_ml:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "539RiG70-TtU",
        "outputId": "2f5cce44-21f9-4f4d-8fdc-96c2a05fb1b4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Multi-label Classification Results:\n",
            "Hamming Loss: 0.1212\n",
            "F1-Score (Sample Averaged): 0.8471\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explanation of the above code\n",
        "This part of the code demonstrates multi-label classification, where each data point (a digit) is associated with multiple binary labels representing different simulated properties. These properties include whether the digit is even, a prime number, less than 5, or greater than 5. By defining custom functions (is_even, is_prime, less_than_5, greater_than_5), these properties are computed for the dataset, and the results are combined into a multi-label target variable y_multilabel.\n",
        "\n",
        "The dataset is then split into training and testing sets, and the features are standardized to ensure consistent scaling. Using OneVsRestClassifier, logistic regression models are trained to predict all labels simultaneously. Predictions are made, and the performance is evaluated with the Hamming Loss (indicating the proportion of incorrect labels) and the sample-averaged F1-score (capturing overall prediction quality).\n",
        "\n",
        "This approach showcases how to handle multi-label classification problems, where each sample can belong to multiple categories. From this, I’ve learned how to engineer multi-label datasets, train models to predict multiple outputs, and evaluate them using appropriate metrics like Hamming Loss and F1-score. It’s a valuable method for complex real-world tasks involving multiple label predictions."
      ],
      "metadata": {
        "id": "tePJuH1J8MVU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What accuracy metric do you use to best capture the accuracy of classification algorithm which predicts the above classes based on some data from each player and why?\n",
        "\n",
        "The choice of accuracy metric depends on the type of classification task you're evaluating:\n",
        "\n",
        "* For multi-class classification (like predicting digits 0–9 in MNIST), metrics such as macro-averaged F1-Score or weighted-averaged F1-Score are ideal. The macro F1-Score gives equal weight to all classes, making it useful when class distribution is balanced. The weighted F1-Score, on the other hand, considers the class distribution, which is helpful when some classes are more frequent than others.\n",
        "\n",
        "* For binary classification (like even vs. odd digits), precision, recall, and F1-Score are often preferred over overall accuracy. These metrics are particularly useful if there is class imbalance, as they focus on how well the model identifies positive and negative cases.\n",
        "\n",
        "* For multi-label classification (like predicting multiple attributes such as even/odd, prime, etc.), Hamming Loss is a good choice as it captures the fraction of incorrect predictions across all labels. Additionally, the sample-averaged F1-Score is valuable for evaluating overall prediction quality across all labels for each sample.\n",
        "\n",
        "Each metric is chosen based on its ability to capture relevant aspects of the model's performance while considering the nature of the classification problem and the data distribution. For instance, in multi-class and multi-label settings, focusing only on accuracy might misrepresent performance, especially when class distributions are imbalanced. Metrics like F1-Score and Hamming Loss provide a more nuanced and reliable evaluation."
      ],
      "metadata": {
        "id": "_wAKxCgL0hRi"
      }
    }
  ]
}