{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "P5iSwvJoFzg0",
        "outputId": "9d249f4b-240d-42ba-a2be-d6d96b288f47"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-0973fc98-596b-4df2-a971-d06d3df4f868\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-0973fc98-596b-4df2-a971-d06d3df4f868\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving diabetes.csv to diabetes.csv\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mohammad Mahdi Razmjoo - 400101272\n"
      ],
      "metadata": {
        "id": "eermfU70GY-A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing and Preparing the Dataset\n",
        "\n",
        "We begin by importing the Pima Indians Diabetes dataset, which you can obtain from [Kaggle](https://www.kaggle.com/datasets/uciml/pima-indians-diabetes-database) or a similar platform. The process starts with exploratory analysis to understand the structure and look for any missing entries. After that, we separate the inputs (features) from the target variable. An 80-20 train-test split is performed, followed by standard scaling—especially essential for algorithms such as KNN and SVM."
      ],
      "metadata": {
        "id": "E7Rl-FeOGfpG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "dataframe = pd.read_csv(\"diabetes.csv\")\n",
        "\n",
        "print(\"Sample records from the dataset:\")\n",
        "print(dataframe.head(), \"\\n\")\n",
        "\n",
        "print(f\"Data dimensions: {dataframe.shape}\\n\")\n",
        "print(\"General information:\")\n",
        "print(dataframe.info(), \"\\n\")\n",
        "\n",
        "print(\"Missing values per column:\")\n",
        "print(dataframe.isnull().sum(), \"\\n\")\n",
        "\n",
        "print(\"Descriptive statistics:\")\n",
        "print(dataframe.describe(), \"\\n\")\n",
        "\n",
        "features = dataframe.drop(\"Outcome\", axis=1)\n",
        "labels = dataframe[\"Outcome\"]\n",
        "\n",
        "train_x, test_x, train_y, test_y = train_test_split(\n",
        "    features, labels, test_size=0.2, stratify=labels, random_state=42\n",
        ")\n",
        "\n",
        "scaler = StandardScaler()\n",
        "train_x_scaled = scaler.fit_transform(train_x)\n",
        "test_x_scaled = scaler.transform(test_x)\n",
        "\n",
        "print(f\"Training set size: {train_x.shape}\")\n",
        "print(f\"Test set size: {test_x.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "semEY7Z6HMvB",
        "outputId": "d6b61731-ffdb-4fcd-8269-0709dc5bb6c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample records from the dataset:\n",
            "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
            "0            6      148             72             35        0  33.6   \n",
            "1            1       85             66             29        0  26.6   \n",
            "2            8      183             64              0        0  23.3   \n",
            "3            1       89             66             23       94  28.1   \n",
            "4            0      137             40             35      168  43.1   \n",
            "\n",
            "   DiabetesPedigreeFunction  Age  Outcome  \n",
            "0                     0.627   50        1  \n",
            "1                     0.351   31        0  \n",
            "2                     0.672   32        1  \n",
            "3                     0.167   21        0  \n",
            "4                     2.288   33        1   \n",
            "\n",
            "Data dimensions: (768, 9)\n",
            "\n",
            "General information:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 768 entries, 0 to 767\n",
            "Data columns (total 9 columns):\n",
            " #   Column                    Non-Null Count  Dtype  \n",
            "---  ------                    --------------  -----  \n",
            " 0   Pregnancies               768 non-null    int64  \n",
            " 1   Glucose                   768 non-null    int64  \n",
            " 2   BloodPressure             768 non-null    int64  \n",
            " 3   SkinThickness             768 non-null    int64  \n",
            " 4   Insulin                   768 non-null    int64  \n",
            " 5   BMI                       768 non-null    float64\n",
            " 6   DiabetesPedigreeFunction  768 non-null    float64\n",
            " 7   Age                       768 non-null    int64  \n",
            " 8   Outcome                   768 non-null    int64  \n",
            "dtypes: float64(2), int64(7)\n",
            "memory usage: 54.1 KB\n",
            "None \n",
            "\n",
            "Missing values per column:\n",
            "Pregnancies                 0\n",
            "Glucose                     0\n",
            "BloodPressure               0\n",
            "SkinThickness               0\n",
            "Insulin                     0\n",
            "BMI                         0\n",
            "DiabetesPedigreeFunction    0\n",
            "Age                         0\n",
            "Outcome                     0\n",
            "dtype: int64 \n",
            "\n",
            "Descriptive statistics:\n",
            "       Pregnancies     Glucose  BloodPressure  SkinThickness     Insulin  \\\n",
            "count   768.000000  768.000000     768.000000     768.000000  768.000000   \n",
            "mean      3.845052  120.894531      69.105469      20.536458   79.799479   \n",
            "std       3.369578   31.972618      19.355807      15.952218  115.244002   \n",
            "min       0.000000    0.000000       0.000000       0.000000    0.000000   \n",
            "25%       1.000000   99.000000      62.000000       0.000000    0.000000   \n",
            "50%       3.000000  117.000000      72.000000      23.000000   30.500000   \n",
            "75%       6.000000  140.250000      80.000000      32.000000  127.250000   \n",
            "max      17.000000  199.000000     122.000000      99.000000  846.000000   \n",
            "\n",
            "              BMI  DiabetesPedigreeFunction         Age     Outcome  \n",
            "count  768.000000                768.000000  768.000000  768.000000  \n",
            "mean    31.992578                  0.471876   33.240885    0.348958  \n",
            "std      7.884160                  0.331329   11.760232    0.476951  \n",
            "min      0.000000                  0.078000   21.000000    0.000000  \n",
            "25%     27.300000                  0.243750   24.000000    0.000000  \n",
            "50%     32.000000                  0.372500   29.000000    0.000000  \n",
            "75%     36.600000                  0.626250   41.000000    1.000000  \n",
            "max     67.100000                  2.420000   81.000000    1.000000   \n",
            "\n",
            "Training set size: (614, 8)\n",
            "Test set size: (154, 8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2) Applying Logistic Regression for Predictive Modeling\n",
        "\n",
        "We now fit a Logistic Regression model to the training data. After training, predictions are made on the test set, and performance is evaluated using the F1-score metric. The goal is to achieve an F1-score greater than 0.75, as specified."
      ],
      "metadata": {
        "id": "dY3HFCUZIw2P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "model_lr = LogisticRegression(random_state=42)\n",
        "model_lr.fit(train_x_scaled, train_y)\n",
        "\n",
        "predictions_lr = model_lr.predict(test_x_scaled)\n",
        "score_f1 = f1_score(test_y, predictions_lr)\n",
        "\n",
        "print(\"Logistic Regression - Test F1 Score:\", score_f1)\n",
        "print(\"\\nClassification Report for Logistic Regression:\")\n",
        "print(classification_report(test_y, predictions_lr))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "050OlpJYI0Ie",
        "outputId": "81f49de0-53f8-412d-dcf6-6b60100eeac4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression - Test F1 Score: 0.56\n",
            "\n",
            "Classification Report for Logistic Regression:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.82      0.79       100\n",
            "           1       0.61      0.52      0.56        54\n",
            "\n",
            "    accuracy                           0.71       154\n",
            "   macro avg       0.68      0.67      0.67       154\n",
            "weighted avg       0.71      0.71      0.71       154\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3) Linear SVM for Classification\n",
        "\n",
        "We proceed by training a Support Vector Machine with a linear kernel using scikit-learn’s SVC. The model is then evaluated on the test data, aiming for an F1-score exceeding 0.80.\n"
      ],
      "metadata": {
        "id": "RfY9nVcfJG3G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "model_svm = SVC(kernel=\"linear\", random_state=42)\n",
        "model_svm.fit(train_x_scaled, train_y)\n",
        "\n",
        "pred_svm = model_svm.predict(test_x_scaled)\n",
        "f1_linear_svm = f1_score(test_y, pred_svm)\n",
        "\n",
        "print(\"Linear SVM - Test F1 Score:\", f1_linear_svm)\n",
        "print(\"\\nClassification Report for Linear SVM:\")\n",
        "print(classification_report(test_y, pred_svm))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CThM0p0rJIcK",
        "outputId": "694ff4f1-24f7-4459-ebdd-8ea28fff4581"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear SVM - Test F1 Score: 0.5656565656565656\n",
            "\n",
            "Classification Report for Linear SVM:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.83      0.79       100\n",
            "           1       0.62      0.52      0.57        54\n",
            "\n",
            "    accuracy                           0.72       154\n",
            "   macro avg       0.69      0.67      0.68       154\n",
            "weighted avg       0.71      0.72      0.71       154\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4) SVM with RBF Kernel\n",
        "\n",
        "Next, we utilize a non-linear Support Vector Machine with an RBF kernel to model potentially complex relationships. A grid search is used to optimize the `C` and `gamma` parameters with the goal of achieving an F1-score above 0.80.\n"
      ],
      "metadata": {
        "id": "WcxjwhLHJYyX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_space = {\n",
        "    'C': [0.1, 1, 10],\n",
        "    'gamma': ['scale', 'auto', 0.01, 0.1, 1]\n",
        "}\n",
        "\n",
        "rbf_svm_model = SVC(kernel='rbf', random_state=42)\n",
        "grid_search = GridSearchCV(rbf_svm_model, param_space, cv=5, scoring='f1', n_jobs=-1)\n",
        "grid_search.fit(train_x_scaled, train_y)\n",
        "\n",
        "optimal_svm_rbf = grid_search.best_estimator_\n",
        "pred_svm_rbf = optimal_svm_rbf.predict(test_x_scaled)\n",
        "f1_rbf_svm = f1_score(test_y, pred_svm_rbf)\n",
        "\n",
        "print(\"Best RBF-SVM parameters:\", grid_search.best_params_)\n",
        "print(\"Kernel SVM (RBF) - Test F1 Score:\", f1_rbf_svm)\n",
        "print(\"\\nClassification Report for Kernel SVM (RBF):\")\n",
        "print(classification_report(test_y, pred_svm_rbf))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zr7yW5nMJWeH",
        "outputId": "02d54dc0-3a1d-432e-c4d5-e1df40069603"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best RBF-SVM parameters: {'C': 10, 'gamma': 0.01}\n",
            "Kernel SVM (RBF) - Test F1 Score: 0.6041666666666666\n",
            "\n",
            "Classification Report for Kernel SVM (RBF):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.87      0.82       100\n",
            "           1       0.69      0.54      0.60        54\n",
            "\n",
            "    accuracy                           0.75       154\n",
            "   macro avg       0.73      0.70      0.71       154\n",
            "weighted avg       0.75      0.75      0.74       154\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5) K-Nearest Neighbors (KNN)\n",
        "\n",
        "## 5.1) Optimizing the Number of Neighbors (k)\n",
        "\n",
        "The performance of KNN depends on the choice of `k` (the number of neighbors). We will conduct a grid search over different values of `k` and weight strategies to maximize the F1-score, with a target of surpassing 0.80."
      ],
      "metadata": {
        "id": "dZAOrNw8JkUd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "knn_param_space = {\n",
        "    'n_neighbors': [3, 5, 7, 9, 11, 13, 15],\n",
        "    'weights': ['uniform', 'distance']\n",
        "}\n",
        "\n",
        "knn_model = KNeighborsClassifier()\n",
        "knn_grid_search = GridSearchCV(knn_model, knn_param_space, cv=5, scoring='f1', n_jobs=-1)\n",
        "knn_grid_search.fit(train_x_scaled, train_y)\n",
        "\n",
        "best_knn_model = knn_grid_search.best_estimator_\n",
        "knn_predictions = best_knn_model.predict(test_x_scaled)\n",
        "f1_knn_score = f1_score(test_y, knn_predictions)\n",
        "\n",
        "print(\"Best KNN parameters:\", knn_grid_search.best_params_)\n",
        "print(\"KNN - Test F1 Score:\", f1_knn_score)\n",
        "print(\"\\nClassification Report for KNN:\")\n",
        "print(classification_report(test_y, knn_predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qZHj1acGJmtV",
        "outputId": "17f18416-d346-444e-e7dc-cc8205188c07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best KNN parameters: {'n_neighbors': 7, 'weights': 'uniform'}\n",
            "KNN - Test F1 Score: 0.6138613861386139\n",
            "\n",
            "Classification Report for KNN:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.84      0.81       100\n",
            "           1       0.66      0.57      0.61        54\n",
            "\n",
            "    accuracy                           0.75       154\n",
            "   macro avg       0.72      0.71      0.71       154\n",
            "weighted avg       0.74      0.75      0.74       154\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6) Decision Trees\n",
        "\n",
        "## 6.1) Optimizing Maximum Depth\n",
        "\n",
        "Decision Trees are sensitive to overfitting. To prevent this, we will adjust hyperparameters like `max_depth`, `min_samples_split`, and `min_samples_leaf` for regularization. The goal is to achieve an F1-score exceeding 0.80.\n"
      ],
      "metadata": {
        "id": "K9WFoER7JuZa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "dt_param_grid = {\n",
        "    'max_depth': [2, 3, 4, 5, 6, 7, None],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 5]\n",
        "}\n",
        "\n",
        "dt_model = DecisionTreeClassifier(random_state=42)\n",
        "dt_grid_search = GridSearchCV(dt_model, dt_param_grid, cv=5, scoring='f1', n_jobs=-1)\n",
        "dt_grid_search.fit(train_x_scaled, train_y)\n",
        "\n",
        "best_dt_model = dt_grid_search.best_estimator_\n",
        "dt_predictions = best_dt_model.predict(test_x_scaled)\n",
        "f1_dt_score = f1_score(test_y, dt_predictions)\n",
        "\n",
        "print(\"Best Decision Tree parameters:\", dt_grid_search.best_params_)\n",
        "print(\"Decision Tree - Test F1 Score:\", f1_dt_score)\n",
        "print(\"\\nClassification Report for Decision Tree:\")\n",
        "print(classification_report(test_y, dt_predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lbIJ729sJxyr",
        "outputId": "f93f50e7-c837-4908-b8e0-c48b9893baa1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Decision Tree parameters: {'max_depth': 4, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
            "Decision Tree - Test F1 Score: 0.693069306930693\n",
            "\n",
            "Classification Report for Decision Tree:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.88      0.85       100\n",
            "           1       0.74      0.65      0.69        54\n",
            "\n",
            "    accuracy                           0.80       154\n",
            "   macro avg       0.78      0.76      0.77       154\n",
            "weighted avg       0.80      0.80      0.80       154\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.2) Regularization Techniques for Decision Trees\n",
        "\n",
        "To reduce the risk of overfitting, we apply the following regularization strategies to decision trees:\n",
        "\n",
        "1. **Limiting Maximum Depth (`max_depth`):**  \n",
        "   By restricting the tree's depth, we reduce its complexity and prevent it from overfitting to noise in the data.\n",
        "\n",
        "2. **Minimum Samples for Splitting and Leaf Nodes (`min_samples_split` and `min_samples_leaf`):**  \n",
        "   These parameters ensure that a node must have a sufficient number of samples before it can be split or form a leaf, which helps avoid creating splits based on very few data points.\n",
        "\n",
        "3. **Cost Complexity Pruning (`ccp_alpha`):**  \n",
        "   This technique prunes the tree after training by penalizing its complexity. Increasing the value of `ccp_alpha` leads to a simpler tree."
      ],
      "metadata": {
        "id": "4iAeK-qxJ7ii"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7) Random Forest\n",
        "\n",
        "Random Forest is an ensemble learning technique that aggregates multiple decision trees. We will optimize key parameters, such as the number of estimators and maximum depth, through grid search, aiming for an F1-score greater than 0.85.\n"
      ],
      "metadata": {
        "id": "WZa91rzrKByU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rf_param_space = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [3, 5, 7, None],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "rf_model = RandomForestClassifier(random_state=42)\n",
        "rf_grid_search = GridSearchCV(rf_model, rf_param_space, cv=5, scoring='f1', n_jobs=-1)\n",
        "rf_grid_search.fit(train_x_scaled, train_y)\n",
        "\n",
        "best_rf_model = rf_grid_search.best_estimator_\n",
        "rf_predictions = best_rf_model.predict(test_x_scaled)\n",
        "f1_rf_score = f1_score(test_y, rf_predictions)\n",
        "\n",
        "print(\"Best Random Forest parameters:\", rf_grid_search.best_params_)\n",
        "print(\"Random Forest - Test F1 Score:\", f1_rf_score)\n",
        "print(\"\\nClassification Report for Random Forest:\")\n",
        "print(classification_report(test_y, rf_predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tDdRdzJ5KC2a",
        "outputId": "757c3a20-a25b-4a40-c7d9-9e151c6b570a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Random Forest parameters: {'max_depth': None, 'min_samples_leaf': 2, 'n_estimators': 100}\n",
            "Random Forest - Test F1 Score: 0.6\n",
            "\n",
            "Classification Report for Random Forest:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.84      0.81       100\n",
            "           1       0.65      0.56      0.60        54\n",
            "\n",
            "    accuracy                           0.74       154\n",
            "   macro avg       0.71      0.70      0.70       154\n",
            "weighted avg       0.73      0.74      0.73       154\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install xgboost"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCEeeualKPIW",
        "outputId": "0094b14e-4842-4777-a97d-8a1c88c71be4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (2.1.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.0.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.21.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from xgboost) (1.14.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8) Bonus: Targeting an F1-Score Above 0.90\n",
        "\n",
        "For the bonus challenge, we aim to surpass an F1-score of 0.90 using XGBoost. This powerful ensemble method often yields improved performance when fine-tuned. If class imbalance is present, techniques such as oversampling might be utilized to enhance model accuracy.\n"
      ],
      "metadata": {
        "id": "Za8_16FdKLFI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "\n",
        "xgb_param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [2, 3, 4, 5],\n",
        "    'learning_rate': [0.01, 0.1, 0.3],\n",
        "    'subsample': [0.8, 1.0]\n",
        "}\n",
        "\n",
        "xgb_model = xgb.XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
        "xgb_grid_search = GridSearchCV(xgb_model, xgb_param_grid, cv=5, scoring='f1', n_jobs=-1)\n",
        "xgb_grid_search.fit(train_x_scaled, train_y)\n",
        "\n",
        "best_xgb_model = xgb_grid_search.best_estimator_\n",
        "xgb_predictions = best_xgb_model.predict(test_x_scaled)\n",
        "f1_xgb_score = f1_score(test_y, xgb_predictions)\n",
        "\n",
        "print(\"Best XGBoost parameters:\", xgb_grid_search.best_params_)\n",
        "print(\"XGBoost - Test F1 Score:\", f1_xgb_score)\n",
        "print(\"\\nClassification Report for XGBoost:\")\n",
        "print(classification_report(test_y, xgb_predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aKW-v_c6KMLq",
        "outputId": "5b9c6f3c-acca-4354-c75b-4a4ea97e610b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best XGBoost parameters: {'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 100, 'subsample': 0.8}\n",
            "XGBoost - Test F1 Score: 0.6415094339622641\n",
            "\n",
            "Classification Report for XGBoost:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.82      0.81       100\n",
            "           1       0.65      0.63      0.64        54\n",
            "\n",
            "    accuracy                           0.75       154\n",
            "   macro avg       0.73      0.72      0.73       154\n",
            "weighted avg       0.75      0.75      0.75       154\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optional: Applying Oversampling to Enhance Performance\n",
        "\n",
        "If the F1-score remains below 0.90, we can implement oversampling techniques, such as Random OverSampling, to address class imbalance and potentially improve model performance.\n"
      ],
      "metadata": {
        "id": "t5U2ZdL9KeEj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "oversampler = RandomOverSampler(random_state=42)\n",
        "X_train_oversampled, y_train_oversampled = oversampler.fit_resample(train_x_scaled, train_y)\n",
        "\n",
        "xgb_grid_search.fit(X_train_oversampled, y_train_oversampled)\n",
        "best_xgb_oversampled = xgb_grid_search.best_estimator_\n",
        "xgb_oversampled_predictions = best_xgb_oversampled.predict(test_x_scaled)\n",
        "f1_xgb_oversampled_score = f1_score(test_y, xgb_oversampled_predictions)\n",
        "\n",
        "print(\"XGBoost with Random Oversampling - Test F1 Score:\", f1_xgb_oversampled_score)\n",
        "print(\"\\nClassification Report for XGBoost with Oversampling:\")\n",
        "print(classification_report(test_y, xgb_oversampled_predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TVdP5ohNKgOU",
        "outputId": "8445776d-b2c2-4ac1-baf3-c247b9372193"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost with Random Oversampling - Test F1 Score: 0.6306306306306306\n",
            "\n",
            "Classification Report for XGBoost with Oversampling:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.78      0.79       100\n",
            "           1       0.61      0.65      0.63        54\n",
            "\n",
            "    accuracy                           0.73       154\n",
            "   macro avg       0.71      0.71      0.71       154\n",
            "weighted avg       0.74      0.73      0.74       154\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9) Summary of All Models' F1-Scores\n",
        "\n",
        "Below, we summarize the F1-scores of all the models to verify if they meet the required performance thresholds:\n",
        "\n",
        "- Logistic Regression: F1-score should be > 0.75  \n",
        "- Linear SVM: F1-score should be > 0.80  \n",
        "- Kernel SVM: F1-score should be > 0.80  \n",
        "- KNN: F1-score should be > 0.80  \n",
        "- Decision Tree: F1-score should be > 0.80  \n",
        "- Random Forest: F1-score should be > 0.85  \n",
        "- Bonus: XGBoost (with or without oversampling): F1-score > 0.90 (if achieved)\n",
        "\n",
        "Note: Due to the nature of the dataset, we may not have been able to reach the desired accuracy levels.\n"
      ],
      "metadata": {
        "id": "nnAf8Ig9KqwD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Logistic Regression (F1):\", score_f1)\n",
        "print(\"Linear SVM (F1):\", f1_linear_svm)\n",
        "print(\"Kernel SVM (F1):\", f1_rbf_svm)\n",
        "print(\"KNN (F1):\", f1_knn_score)\n",
        "print(\"Decision Tree (F1):\", f1_dt_score)\n",
        "print(\"Random Forest (F1):\", f1_rf_score)\n",
        "print(\"XGBoost (F1):\", f1_xgb_score, \"(Bonus attempt)\")\n",
        "print(\"XGBoost with Oversampling (F1):\", f1_xgb_oversampled_score, \"(Optional bonus attempt)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O5yotOzTKss1",
        "outputId": "877fe141-d7ac-4b81-96c1-2eba3d19fcec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression (F1): 0.56\n",
            "Linear SVM (F1): 0.5656565656565656\n",
            "Kernel SVM (F1): 0.6041666666666666\n",
            "KNN (F1): 0.6138613861386139\n",
            "Decision Tree (F1): 0.693069306930693\n",
            "Random Forest (F1): 0.6\n",
            "XGBoost (F1): 0.6415094339622641 (Bonus attempt)\n",
            "XGBoost with Oversampling (F1): 0.6306306306306306 (Optional bonus attempt)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion\n",
        "\n",
        "In this notebook, we presented a complete binary classification pipeline applied to the Pima Indians Diabetes Dataset:\n",
        "\n",
        "- **Data loading, exploratory data analysis (EDA), and preprocessing** with scaling.\n",
        "- **Implementation and tuning** of models including **Logistic Regression, Linear SVM, Kernel SVM, KNN, Decision Trees, and Random Forests**.\n",
        "- Discussion of **regularization methods for Decision Trees**.\n",
        "- A **bonus challenge** with XGBoost (both with and without oversampling), aimed at achieving an F1-score above 0.90 on the test set.\n",
        "\n",
        "Each section is accompanied by explanations that detail the rationale behind each step and the insights gained."
      ],
      "metadata": {
        "id": "FGoiTkRqKvDw"
      }
    }
  ]
}